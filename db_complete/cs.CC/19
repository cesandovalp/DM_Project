effici learn linear separ bound nois pranjal awasthi pawashti maria florina balcan ninamf nika haghtalab nhaghtal ruth urner rurner march abstract studi learnabl linear separ presenc bound massart nois realist general random classif nois model adversari flip exampl xwith probabl provid polynomi time algorithm learn linear separ arbitrarili small excess error nois model uniform distribut unit ball constant studi statist learn theori communiti context faster converg rate comput effici algorithm model remain elus work evid design algorithm achiev arbitrarili small excess error polynomi time realist nois model open excit addit provid lower bound show popular algorithm hing loss minimiza tion averag lead arbitrarili small excess error massart nois uniform distribut work margin base techniqu develop context activ learn result algorithm activ learn algorithm label complex logarithm desir excess error introduct overview linear separ popular classifi studi theori practic machin learn design nois toler polynomi time learn algorithm achiev arbitrarili small excess error rate linear separ long stand question learn theori absenc nois data realiz algorithm exist linear program problem harder presenc label nois work concern design algorithm achiev error opt arbitrarili close opt error best linear separ time polynomi usual call excess error strong guarante well studi random classif nois model work provid algorithm achiev arbitrarili small excess error polynomi time bound nois call massart nois realist studi nois model statist learn theori addit strong lower bound nois model comput effici learn algorithm hing loss minim averag algorithm independ interest motiv work comput effici algorithm learn halfspac focus extrem hand styliz random classif nois model rcn exampl flip independ equal probabl work provid comput effici algorithm achiev arbitrarili small excess error polynomi time note crucial exploit high amount symmetri rcn nois extrem work difficult adversari nois model includ agnost model malici nois model best requir addit distribut assumpt margin instanc space achiev weaker multipl approxim guarante exampl best result form case uniform distribut unit sphere achiev excess error copt larg constant interest technic point view guarante form troubl statist point view inconsist sens barrier opt prove excess error decreas sampl fact evid unavoid polynomi time algorithm adversari nois model work identifi realist studi nois model statist learn theori call massart nois prove stronger guarante massart nois thought general random classif nois model label exampl flip independ probabl adversari control choos nois rate exampl constraint statist point view well model faster rate compar worst case joint distribut comput learn theori nois model studi malici misclassif nois high unsymmetr natur comput effici learn algorithm model remain elus work provid comput effici algorithm achiev arbitrarili small excess error learn linear separ formal exist polynomi time algorithm learn linear separ error opt poli under distribut uniform distribut unit ball nois exampl upper bound constant independ dimens mention earlier result form random classif nois technic point view oppos random classif nois error classifi scale uniform observ label observ error classifi masasart nois chang drastic monoton fashion fact adversari control choos nois rate exampl result work standard algorithm averag algorithm work random nois achiev poorer excess error function massart nois technic speak fact massart nois introduc high correl observ label compon orthogon direct best classifi face challeng entir approach consid random classif nois analyz margin base algorithm algorithm design learn linear separ agnost malici nois model achiev excess error copt constant structur insight exist constant independ dimens massart nois flip probabl upper bound modif algorithm achiev arbitrarili small excess error result defin adapt chosen sequenc hing loss minim problem smaller smaller band current guess target relat hing loss loss care local analysi will direct closer closer optim classifi allow achiev arbitrarili small excess error rate polynomi time algorithm adapt chosen sequenc hing loss minim problem wonder guarante shot hing loss minim provid strong negat result noisi distribut satisfi massart nois paramet hing loss minim return classifi excess error result independ interest exist earlier work show hing loss minim lead classifi larg loss lower bound paper employ distribut mass discret point flip label massart nois larg distanc optim classifi result strong hing sensit error larg distanc hing loss minim bound fail benign condit appeal featur result algorithm analyz fact natur adapt activ learn select sampl scenario intens studi year learn algorithm receiv classif exampl model algorithm achiev label complex depend error paramet polylogarithm exponenti better passiv algorithm polynomi time activ learn algorithm learn linear separ massart nois note prior work ineffici algorithm achiev desir label complex massart nois work agnost nois model notori hard deal comput evid achiev arbitrarili small excess error polynomi time hard model model distribut assumpt algorithm learn linear separ excess error run time poli dexp work evid exponenti depend unavoid case agnost case side step consid structur realist nois model motiv fact modern machin learn applic massiv amount unanno tate unlabel data interest design activ learn algorithm effici util data minim human intervent decad substanti progress understand under statist principl activ learn general character develop describ activ learn advantag classic passiv supervis learn paradigm nois free set agnost case despit effort simpl nois model random classif nois linear nois comput effici algorithm provabl guarante presenc massart nois achiev arbitrarili small excess error note work comput effici algorithm passiv activ learn assumpt hing loss surrog loss minim align mini mizer loss work case massart nois margin instanc space uniform provid comput effici algorithm challeng set preliminari consid binari classif problem work problem predict binari label instanc assum data point drawn unknown under distribut instanc space label space purpos work consid distribut margin uniform distribut dimension unit ball work class homogen halfspac denot byh sign halfspac defin error respect errd sign examin learn halfspac presenc massart nois set assum bay optim classifi linear separ note error massart nois paramet condit condit label probabl equival satisfi massart nois paramet adversari construct distribut instanc sign flip label instanc probabl note distribut remain bay optim classier remaind work refer noisi distribut distribut instanc sign clean distribut goal find halfspac small excess error compar bay optim classifi find halfspac errd errd note excess error classifi depend point region disagre errd errd addit massart nois amount nois disagr region bound difficult massart nois errd errd analysi frequent examin region margin halfspac halfspac margin set point fall margin distribut indic distribut condit remaind work refer region band analysi hing loss convex surrog function loss halfspac normal hing loss defin max wÂ·x label sampl set empir hing loss vector respect comput effici algorithm massart nois prove main result learn half space presenc massart nois focus case uniform distribut dimension unit ball main theorem theorem optim bay classifi half space denot assum massart nois condit hold algorithm run polynomi time proceed log round round take poli exp log unlabel sampl log label probabl return linear separ excess error compar relationship massart nois paramet maximum flip probabl discuss intro duction note theorem algorithm unknown algorithm adapt valu accept rang defin theorem algorithm describ iter margin base approach algorithm run log round constant induct assum algorithm produc hypothesi round satisfi base case algorithm round sampl label exampl condit distribut uniform distribut choos set hypothesi minim empir hing loss exampl subsequ prove detail note excess error error label correct errd errd errd uniform errd impli excess error algorithm describ origin introduc achiev error err con stant presenc adversari nois achiev small excess error err ambiti goal requir technic insight crucial technic innov follow key observ massart nois nois rate condit distribut focus distribut band nois rate increas second technic contribut care choic paramet choic paramet upto constant play role toler constant amount massart nois insight algorithm achiev stronger guarante arbitrarili small excess error presenc massart nois algorithm achiev error err presenc massart nois algorithm effici algorithm arbitrarili small excess error massart nois input distribut oracl return oracl return sampl permit excess error probabl failur paramet learn rate sequenc sampl size sequenc angl hypothesi space sequenc width label space sequenc threshold hing loss algorithm poli sampl poli time algorithm find half excess error refer appendix draw exampl work set log find result minim empir ical hing loss threshold minw clear work set normal addit exampl draw exampl reject output return excess error probabl overview analysi divid errd categori error band swk error band swk choos hypothesi consid step probabl mass band disagre small lemma error associ region band small motiv design algorithm minim error band probabl mass band small errd hold suffic small constant error clean distribut restrict band dwk minim hing loss band minim loss hard altern method find small error band need hing loss convex loss function effici minim effici find minim empir hing loss sampl drawn allow hing loss remain faith proxi loss focus band smaller width normal hing loss function defin max crucial analysi involv show minim empir hing loss sampl set drawn small error dwk proport hing loss dwk upper bound error band small lemma notic massart nois nois rate margin distribut focus distribut band increas probabl nois band nois point band close decis boundari intuit speak increas hing loss insight hing loss close hing loss dwk lemma proof theorem lemma prove theorem introduc seri lemma concern behavior hing loss band lemma build show error fix small constant band eas exposit repres dwk repres lemma proof appear appendix upper bound true hing error clean distribut band lemma lemma compar true hing loss distribut clear differ hing loss distribut entir attribut nois point margin key insight proof lemma concentr band probabl nois point remain fact massart nois label chang probabl concentr band point close decis boundari close angl point band close decis boundari hing loss nois point band increas total hing loss lemma proof set nois point sign cauchi shwarz definit uniform label sampl set drawn random clean set sampl label correct clean sign standard dimens bound proof includ appendix log random drawn set label sampl probabl clean lemma crucial step analysi algorithm lemma prove ifwk minim empir hing loss sampl drawn noisi distribut band high probabl small error respect clean distribut band dwk lemma exist log random drawn label sampl set size minimum empir hing loss set hypothesi probabl errdk proof sketch note true error distribut true hing loss distribut lemma upper bound true hing loss distribut remain creat connect empir hing loss sampl drawn true hing loss distribut achiev general bound equat connect empir true hing loss lemma connect hing clean noisi distribut proof theorem eas exposit recal note excess error error clean distribut errd errd errd uniform distribut errd excess error suffic errd goal achiev excess error round indirect bound errd step induct algorithm adversari nois model achiev excess error errd log refer appendix detail massart nois errd choic algorithm achiev excess error poli sampl time equat assum round errd will chosen algorithm round errd note errd impli swk indic band round divid error part error band error insid band errd error band prx inequ hold applic lemma fact second error insid band errdk errdk lemma errdk pid transit hold fact replac upper bound errdk lemma errd suffic inequ hold pid simplifi inequ replac valu errd sampl complex analysi requir label sampl band swk round lemma probabl random drawn sampl fall swk leasto unlabel sampl exampl band probabl total unlabel sampl complex log log log averag work algorithm describ previous convex loss minim case hing loss band effici proxi minim loss averag algorithm introduc comput effici algorithm provabl nois toler guarante nois model distribut exampl achiev arbitrarili small excess error presenc random classif nois monoton nois distribut uniform unit sphere presenc small amount malici nois symmetr distribut averag weak learner boost achiev trivial nois toler natur nois toler averag exhibit extend case massart nois uniform distribut answer question negat lack symmetri massart nois present barrier shot applic averag margin distribut complet symmetr addit discuss obstacl incorpor averag weak learner margin base techniqu nutshel averag takesm sampl point respect label return iyi main result wide rang distribut symmetr natur includ gaussian uniform distribut instanc massart nois averag achiev arbitrarili small excess error theorem continu distributiond function distanc origin noisi distribut satisfi massart nois condit equat paramet averag return classifi excess error proof target halfspac nois distribut flip label probabl label clear satisfi massart nois paramet expect vector return averag angl equat larg excess error examin expect compon parallel eas exposit divid analysi case region nois third quadrant second region nois second fourth quadrant event symmetri easi term label chang second term label point stay probabl flip probabl expect parallel compon examin orthogon compon second coordin previous case clean region second quadrant noisi fourth quadrant second quadrant symmetri arctan equat errd errd margin base analysi reli hing loss minim band round effici find halfspac weak learner errdk small constant demonstr lemma motiv lenient goal find weak learner averag effici algorithm find low error halfspac incorpor margin base techniqu hing loss minim argu margin base techniqu inher incompat averag margin base techniqu maintain key properti step angl angl small result small second weak learner errdk small constant work hing loss minim band guarante properti simultan limit search halfspac close angl limit distribut dwk case averag concentr band dwk bias distribut orthogon compon respect upper bound serv assur data orthogon well inform speak lose signal direct direct formal consid construct theorem distribut dwk compon parallel scale width band probabl stay band pass origin log concav includ gaussian uniform distribut orthogon compon remain unchang hing loss minim work hing loss minim techniqu machin learn surpris hing loss minim lead arbitrarili small excess error small nois condit consist note set massart nois consist achiev arbitrarili small excess error bay optim classifi member class halfspac earlier hing loss minim lead classifi larg loss lower bound paper employ distribut mass discret point flip label massart nois larg distanc optim classifi result strong hing sensit error larg distanc hing loss minim bound fail benign condit concret paramet arbitrarili small bound probabl flip label hing loss minim consist distribut uniform margin unit ball bay optim classifi halfspac nois satisfi massart nois condit bound exist constant sampl size hing loss minim return classifi excess error high probabl sampl size hing loss minim approxim optim hing loss translat agnost learn guarante halfspac respect loss small nois condit class distribut uniform margin unit ball bay classifi halfspac satisfi massart nois condit paramet lower bound hing loss minim state theorem hing loss paramet massart nois paramet exist distribut distribut uniform margin satisfi massart condit hing loss minim consist respect class halfspac exist sampl size hing loss minim will output classifi excess error larger high probabl sampl size proof idea prove result defin subclass consist well structur distribut hing paramet bound nois distribut hing loss minim consist figur remaind notat classifi asso ciat vector sign geometr construct conveni differenti defin famili distribut index angl nois paramet bay optim classifi linear unit vector classifi defin unit vector angl partit unit ball area figur consist wedg disagr wedg classifi agre divid point closer point closer flip label point probabl leav label determinist area formal point angl point angl label condit label probabl point label probabl probabl clear distribut satisfi massart nois condit equat paramet goal construct design distribut vector direct smaller hing loss direct observ nois will tend differ hing loss symmetr respect direct nois area will help point area closer hyperplan defin defin vector will pay hing loss nois area correspond area point closer hyperplan defin defin add nois cost area small nois level vector direct expect hing minim argu hing loss minim will approxim arbitrarili close angl achiev arbitrarili small excess error arbitrarili small bound nois hing paramet choos angl hing loss minim consist distribut detail proof appendix conclus work provid comput effici algorithm massart nois model distribut assumpt identifi statist learn yield fast statist rate converg comput statist effici crucial machin learn applic comput statist complex studi dispar set assumpt model view comput complex learn massart nois step bring line closer hope will spur work identifi situat lead comput statist effici ultim light under connect depend aspect autom learn acknowledg work support nsf grant ccf ccf ccf sloan fellowshp microsoft faculti fellowship googl award refer sanjeev arora szlo babai jacqu stern sweedyk hard approxim optima lattic code system linear equat proceed ieee annual symposium foundat comput scienc foc pranjal awasthi maria florina balcan philip long power local effici learn linear separ nois proceed annual acm symposium theori comput stoc maria florina balcan alina beygelzim john langford agnost activ learn proceed ing intern confer machin learn icml maria florina balcan andrei broder tong zhang margin base activ learn proceed ing annual confer learn theori colt maria florina balcan vitali feldman statist activ learn algorithm advanc neural process system nip shai ben david david loker nathan srebro karthik sridharan minim misclassif error rate surrog convex loss proceed intern confer machin learn icml avrim blum alan friez ravi kannan santosh vempala polynomi time algorithm learn noisi linear threshold function algorithmica karl heinz borgwardt simplex method volum algorithm combinator studi text springer verlag berlin olivi bousquet ste phane boucheron gabor lugosi theori classif survey advanc esaim probabl statist rui castro robert nowak minimax bound activ learn proceed annual confer learn theori colt nello cristianini john shaw taylor introduct support vector machin kernel base learn method cambridg univers press amit dani nati linial shai shalev shwartz averag case complex improp learn complex proceed annual acm symposium theori comput stoc sanjoy dasgupta coars sampl complex bound activ learn advanc neural infor mation process system nip sanjoy dasgupta activ learn encyclopedia machin learn sanjoy dasgupta daniel hsu clair monteleoni general agnost activ learn algorithm advanc neural process system nip ofer dekel claudio gentil karthik sridharan select sampl activ learn singl multipl teacher journal machin learn yoav freund sebastian seung eli shamir naftali tishbi select sampl queri committe algorithm machin learn venkatesan guruswami prasad raghavendra hard learn halfspac nois proceed annual ieee symposium foundat comput scienc foc steve hannek bound label complex agnost activ learn proceed intern confer machin learn icml steve hannek theori disagr base activ learn foundat trend machin learn steve hannek liu yang surrog loss passiv activ learn corr adam tauman kalai adam klivan yishay mansour rocco servedio agnost learn ing halfspac siam journal comput adam tauman kalai yishay mansour elad verbin agnost boost pariti learn proceed annual acm symposium theori comput stoc michael kearn ming learn presenc malici error extend abstract proceed annual acm symposium theori comput stoc michael kearn robert schapir linda selli effici agnost learn pro ceed annual confer comput learn theori colt adam klivan pravesh kothari embed hard learn problem gaussian space approxim random combinatori optim algorithm techniqu prox random adam klivan philip long rocco servedio learn halfspac malici nois journal machin learn pascal massart lodi ndlec risk bound statist learn annal statist ronald rivest robert sloan formal model hierarch concept learn comput rocco servedio effici algorithm comput learn theori harvard univers robert sloan pac learn nois geometri learn geometri comput approach springer probabl lemma uniform distribut probabl lemma work variat lemma present previous work term asymptot behavior focus find bound tight constant concern improv constant bound essenti toler massart nois uniform distribut dimension ball indic volum dimension unit ball ratio volum unit ball dimens common find probabl mass region uniform distribut note bound prove analysi lemma upper lower bound probabl mass band uniform distribu tion lemma unit vector proof upper bound note integr prx lower bound note integr assum prx lemma unit vector sign sign proof loss general assum cos sin consid project coordin event interest consid case symmetr sin sin consid circl radius center indic arc circl length arc arc length fall disagr region minus arc length fall band width note sign sign chang variabl proof margin base lemma proof lemma note choic lemma note maxim prx numer erf taylor expans inequ fact choic paramet proof lemma note convex loss minim procedur return vector nec essarili normal consid vector step optim vector length result errdk lemma equat minim hing loss equat lemma lemma lemma constant log random drawn set label sampl probabl clean proof lemma swk result appli lemma initi initi margin base procedur algorithm guarante mention hold long nois rate explicit comput constant easi check comput inequ proof lemma term lower bound second term second term upper bound impli hing loss minim hing loss minim consist setup lead arbitrarili small excess error denot unit ball will work set recal hing loss vector exampl defin max distribut denot expect hing loss clear context omit superscript write algorithm minim empir hing loss sampl argminw hing loss minim halfspac converg optim hing loss halfspac hing loss consist sampl size distribut minw translat agnost learn guarante halfspac respect loss hing loss minim consist respect loss restrict benign class distribut class distribut uniform margin unit ball bay classifi halfspac satisfi massart nois condit paramet distribut sampl size hing loss minim will output classifi excess error larger expect sampl size larger precis minw errd formal lower bound hing loss minim state theorem restat hing loss paramet massart nois paramet exist distribut distribut uniform margin satisfi massart condit hing loss minim consist respect class halfspac exist sampl size hing loss minim will output classifi excess error larger high probabl sampl size notat classifi associ vector sign geometr construct conveni differenti rest devot prove theorem class distribut figur defin famili distribut index angl nois paramet margin uniform unit ball bay optim classifi linear unit vector classifi defin unit vector angl partit unit ball area figur consist wedg disagr wedg classifi agre divid point closer point closer add nois point area leav label determinist area formal point angl point angl label condit probabl point label probabl probabl lemma lemma relat hing loss unit length vector hing loss arbitrari vector unit ball will allow focus attent compar hing loss unit vector argu hing loss vector arbitrari norm lemma letw andw vector unit length proof definit hing loss max max lemma denot halfspac minim hing loss respect hing loss minim consist loss proof hing loss minim vector note consid case hing loss strict smaller integr hing loss unit ball polar coordin sin sin sin sin sin sin case hing minim assumpt lemma posit angl continu function choos note set compact choos angl vector angl min hing loss minim will eventu expect larg sampl output classifi hing loss strict smaller will output classifi angl smaller equat errd errd excess error classfier return hing loss minim lower bound constant Âµpi hing loss minim consist respect loss proof theorem will bound nois unit length vector strict lower hing loss unit length vector lemma impli bound nois impli hing minim multipl posit angl lemma tell hing loss minim consist loss figur sequel will focus unit length vector choos function denot hing loss wedg half area label correct hing loss area label correct analog defin exampl integr hing loss unit ball polar coordin sin sin sin sin sin sin express hing loss term quantiti note area relat area relat vice versa role exchang exampl noisi version area classifi pay yield figur area defin area point angl figur defin analog note smaller hing loss signifi amount nois onward will smaller hing loss choos small choos angl small area includ band sin sin cos cos area consid case separ sin sin cos sin cos cos sin call quantiti depend cos cos sin observ will yield condit angl bound allow nois choos small distribut consid case case lower bound sin sin sin figur area provid upper bound integr triangular shape figur note bound exact cos strict upper bound cos tan tan tan tan sin tan yield case cos cos sin tan call quantiti differenti easi fix choos angl small argu will fix angl function smaller grow suffic tan monoton increas tan tan small monoton increas desir summar choos small complet proof 